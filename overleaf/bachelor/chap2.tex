\chapter{Related work}
\pagestyle{fancy}
% \lhead{Related work}
\label{relatedwork}

\section {Data mining techniques}

One search made for the key-word “strava” produced 201 relevant results on ScienceDirect, 84 on Scopus, 52 on Web of Science and 4 on IEEE Xplore. Aside from that, 313307 results were produced by searching  "data mining" articles. Comparatively, key-words such as “fitness”, “running” or “cycling”, “weather influence on sports” triggered almost no paper, if any, describing data mining algorithms that can be applied on data collected from mundane physical activities. Furthermore, I found no relevant results (papers, documents or books) on analysing the influence of weather on fitness habits in the context of computational systems. Those being said, this proposal has a solid background of research and engineering tools, yet almost no research paper, thus I find it worth investigating. “The number of interactive fitness technologies, applications and networks which have gamified and biomedicalized real-world activities such as cycling have increased significantly over recent years” \cite{NOBAKHT2020102509}.

The business layer behind those applications encloses one or more techniques of data mining, that return an appropriate response for the circumstances and the goal, openly adapting to changes in both the environment and also the goal, learning from experience and making the appropriate choice given the perceptual limitations and finite computation \cite{10.5555/1809744}. In the following sections I will consider that a system capable of identifying the weather influences over fitness habits is enclosed in the category of the intelligent agents. Until the end of this paper I will describe methods of extracting information in the context of this subject.

\begin{figure}[ht!]
    \centering
    \includegraphics[width = 12cm]{figures/dmt}
    \caption{Data mining trend from 2000-2011 \cite{LIAO201211303}}
    \label{fig:dmt}
\end{figure}

\subsection{Strategies}

"Supervised learning methods are deployed (or strategically put into service in an information technology context) when values of variables (inputs) are used to make predictions about another variable (target) with known values. Unsupervised learning methods can be used in similar situations, but are more frequently deployed on data for which a target with known values does not exist. An example of a supervised method would be a healthcare organization finding out through predictive modeling what attributes distinguish fraudulent claims. In supervised methods, the models and attributes are known and are applied to the data to predict and discover information. With unsupervised modeling, the attributes and models of fraud are not known, but the patterns and clusters of data uncovered by data mining can lead to new discoveries." \cite{hoho}

\begin{table}[ht!]
  \begin{center}
    \label{tab:table9}
    {\begin{tabular}{l|l|l}
      \textbf{Modeling Objective} & \textbf{Supervised} & \textbf{Unsupervised}\\
      \hline
       Prediction & Ordinary least squares regression & Not feasible \\
       & Logistic regression &\\
       & Neural networks &\\
       & Decision trees &\\
       & Memory-based reasoning &\\
       & Support vector machines &\\
       & Multi-adaptive regression splines &\\
       Classification & Decision trees & Clustering (K means) \\
        & Neural networks & Kohonen networks \\
        & Discriminant analysis & Self-organizing \\
        & Bagging and boosting ensembles & \\
        & Naive Bayes classifiers & \\
       Exploration & Decision trees & Principal components \\
       & & Clustering (K means) \\
       & & Link analysis \\
    \end{tabular}}
  \end{center}
\end{table}

"Prediction algorithms determine models or rules to predict continuous or discrete target values given input data. For example, a prediction problem could involve attempting to predict the amount of an insurance claim or a death rate given a set of inputs (pick one and then list the corresponding inputs). Classification algorithms determine models to predict discrete values given input data. A classification problem might involve trying to determine whether a particular purchase represents anomalous behavior based on some indicators (eg, where the purchase was made, the amount of the purchase, or the type of purchase). Exploration uncovers dimensionality in input data. Trying to identify groups of similar customers based on spending habits for a large, targeted mailing is an exploration problem. Affinity analysis determines which events are likely to occur in conjunction with one another. Retailers use affinity analysis to analyze product purchase combinations in grocery stores." \cite{hoho}

\subsubsection{Similar approach for data mining bike sharing demand prediction in metropolitan city}

In \cite{E2020353}, the authors are discussing a couple of models for hourly rental bike demand prediction. An interesting aspect of this paper is that it uses similar weather measurements (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information. The paper also explores "a filtering of features approach to eliminate the parameters which are not predictive and ranks the features based on its prediction performance. Five Statistical regression models were trained with their best hyperparameters using repeated cross-validation and the performance is evaluated using a testing set: (a) Linear Regression (b) Gradient Boosting Machine (c) Support Vector Machine (Radial Basis Function Kernel) (d) Boosted Trees, and (e) Extreme Gradient Boosting Trees. When all the predictors are employed, the best model Gradient Boosting Machine can give the best and highest R2 value of 0.96 in the training set and 0.92 in the test set" \cite{E2020353}.

\subsubsection{Understanding pattern's discovery in drug trials using SEMMA}

Research made in the area of drugs discovery can be accelerated with real time data exploration as the throughput of candidates for clinical trials have significantly increased in the last couple of years, and the time required for drugs preparation decreased. Using streaming tools also alleviate the burden of high throughput. "Twenty years ago, pharmaceutical scientists could screen approximately 30 drug candidates per week. Today, with combinatorial chemistry and high throughput screening, the same scientists can screen thousands of compounds per day in a controlled biological assay to determine potentially effective treatments. The most effective compounds, called “hits,” are classified and retested. The best “leads” may result in a chemical synthesis program to optimize the chemical structure regarding the biological activity of interest" \cite{hoho}. A scientist can study the relationship between chemical structures and biological activity in a real-time manner. After patterns were discovered, the next step is finding the cause of those patterns. "Numerous modeling techniques are available, including traditional statistics. Some market analysts may include visualization as a modeling technique, but the SEMMA method finds an earlier place for this in the exploration phase" \cite{hoho}. It stands for Sample, Explore, Modify, Model, and Assess, a list of sequential steps used for guiding the implementation of data mining applications.


\begin{table}[ht!]
  \begin{center}
    \label{tab:table5}
    {\begin{tabular}{p{2cm}|p{12cm}}
        \textbf{Step} & \textbf{Description} \vspace{2mm} \\
        \hline
         Sample & Mining a representative sample of the initial population decreases the processing time required to begin the knowledge exploration. Estimating population parameters given a representative sample is a well established topic in statistics. If patterns appear in the data as a whole, these will be traceable in a representative sample as well. In \cite{hoho} the goal is to relate chemical structure to biological activity. Because nobody can know how well the results are going to be generalized to other data sets, a common way is to create partitions of the given data set. Partitions can be generated as data comes into the system. \vspace{2mm} \\
         Explore & Visualization is one of the most versatile techniques when exploring large data sets. Most textbooks and computer algorithms on statistical methods will introduce the following ideas: numerical calculations are precise, but graphs are rough; there exists only one set of calculations constituting a correct data analysis; "performing intricate calculations is virtuous, whereas actually looking at the data is cheating". Back in Feb., 1973, F. J. Anscombe was advocating the usefulness of graphs \cite{anscombe}, being essential to good statistical analysis. Scientists often tend to overlook the computations while missing the bigger picture. "There are graphical and analytical means of exploration in data mining, and visualization is one of the most versatile techniques" \cite{hoho} In the drug discovery examples described above, scientists graphically examined different variables. Even though it was not described in the theoretical section, Google Cloud also provides tools for graphically summarization. \vspace{2mm}\\
         Modify & Data quality is essential in this problem and it might have records with missing values or outliers that could obstruct some of the patterns. Removing those records can be a solution, but instead of removing data we can replace them with a mean, a median, or user-specified method.\vspace{2mm} \\
         Model & Because not all methods are suitable for all data, we should choose those that harness knowledge without bending the problem in order to fit a particular method. \vspace{2mm}\\
         Assess & In the preliminary stages of the study neural networks seemed to be the best model, having better results than the logistic regression and the decision tree, capturing more of the best compounds more quickly. Even so, a later examination of scores revealed that the neural network did better than the logistic regression but not as well as the decision tree in identifying active compounds. "Thus, in the process flow diagram, the output model was changed from neural networks to decision tree so that the tree-based model will be used for subsequent predictions." \cite{hoho} \\
         
    \end{tabular}}
  \end{center}
\end{table}

\clearpage
\subsection{Drawbacks and overcomes}

\subsubsection{Misleading sensors values}

\subsection{Summation} 

Working on it.